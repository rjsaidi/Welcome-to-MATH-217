---
title: "Chapter 11-12 - Statistical Inference"
author: "Rachel Saidi"
format: docx
editor: visual
prefer-html: true
---

## Load library, set the working directory, and dataset

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(tidymodels)
setwd("C:/Users/rsaidi/Dropbox/Rachel/MontColl/Datasets/Datasets")
sex_disc <- read_csv("sex_discrimination.csv")
set.seed(9753)  # allows to replicate results each time document is rendered
```

## This vignette will help you understand HT using tidymodels

https://www.tidymodels.org/learn/statistics/infer/#:~:text=hypothesize()%20allows%20you%20to,to%20form%20the%20null%20distribution. 

## Two-Way table

```{r}
decision_table  <- table(sex_disc)

kbl(decision_table, caption = "Two-Way Table") |> 
  kable_styling() |> 
  row_spec(row = 0, color = "dodgerblue")

kbl(round(proportions(decision_table, "sex"),3)) |> # marginal proportions by sex for promotion decision rounded to 3 places
  kable_styling()|>
  row_spec(row = 0, color = "dodgerblue")
```

We can see that 10/24 (41.7%) women were not promoted and 3/24 (12.5%) men were not promoted. The question is, does this appear to be clear bias, or are the differences in proportions due to random chance?

## Random permutations

### One random permutation

Using the mutate() and sample() functions, the vector of promotion decision is mixed up, or permuted, such that whether someone is male or female can't possibly be causing any difference in proportions. However, due to inherent natural variability, there is also no expectation that the promotion decisions are exactly the same for any sample. We use the sample() function to create the shuffled promotion decisions, and then save that shuffled dataset into a new variable named decision_perm, using the mutate() function.

```{r}
perm1 <- sex_disc |>
  mutate(decision_perm = sample(decision)) |>
  group_by(sex) |>
  summarize(prop_promoted_perm = mean(decision_perm == "promoted"),
            prop_promoted = mean(decision == "promoted")) |>
  summarize(diff_perm = diff(prop_promoted_perm), 
            diff_orig = diff(prop_promoted))  # promoted - not promoted
kbl(perm1)|>
  kable_styling() |>
  row_spec(row = 0, color = "dodgerblue")
```

We can see that the original (observed) difference in proportions was 41.7%-12.5%=29.2%

The permutated sample difference in proportions was different. Notice it will change each time you run this chunk due to random chance.

### Many random permutations

By repeating the permutation and difference calculations five times, the permuted differences are seen to be sometimes positive, sometimes negative, sometimes close to zero, sometimes far from zero. However, five times isn't quite enough to capture all of the variability in the null differences.

The rep_sample_n() function performs repeated sampling of a dataset, where the size of the sample is specified in the first argument and the number of repetitions is specified in the reps argument. We notice that the sample size is the same size as the sex discrimination dataset, since it equals the number of rows in the original dataset. We are also specifying that the sampling of the datset should be done without replacement (replace = FALSE), since we want each row to only be selected once. You can think of this as creating five copies of the original sex discrimination dataset.

```{r}
perm_reps <- sex_disc |>
  rep_sample_n(size = nrow(sex_disc), reps = 10, replace = FALSE) |>
  mutate(decision_perm = sample(decision)) |>
  group_by(replicate, sex) |>
  summarize(prop_promoted_perm = mean(decision_perm == "promoted"),
            prop_promoted = mean(decision == "promoted")) |>
  summarize(diff_perm = diff(prop_promoted_perm), 
            diff_orig = diff(prop_promoted))  # promoted - not promoted
kbl(perm_reps) |>
  kable_styling() |>
    row_spec(row = 0, color = "dodgerblue")
```

*Notice the variation in the permutation differences.*


```{r}
ggplot(perm_reps, aes(diff_perm))+
  geom_histogram(bins = 5)
```

## Repeat the permutation 1000 times and plot all differences

```{r}
perm_1000_reps <- sex_disc |>
  rep_sample_n(size = nrow(sex_disc), reps = 1000, replace = FALSE) |>
  mutate(decision_perm = sample(decision)) |>
  group_by(replicate, sex) |>
  summarize(prop_promoted_perm = mean(decision_perm == "promoted"),
            prop_promoted = mean(decision == "promoted")) |>
  summarize(diff_perm = diff(prop_promoted_perm), 
            diff_orig = diff(prop_promoted))  # promoted - not promoted
```

## Create a dotplot of the replications of permutated differences

Show the cut-off for +/- the original difference in proportions of "promoted"

```{r}
plot_perm <- ggplot(perm_1000_reps, aes(diff_perm)) +
  geom_dotplot(binwidth = 0.01) +
  geom_vline(xintercept = c(-0.292, 0.292), color = "blue", linetype = "dotted", linewidth=1) +
  labs(title = "Dotplot of 100 Replications of Permutations\n of Differences of Proportions")
plot_perm
```

Remember that each dot represents a different permutation of the differences in proportions of promotions between males and females.

Although the plot will appear differently each time due to randomization, notice that very few points are \> 0.292 or \< -0.292. Visually, this shows that it is less likely that the difference of 29.2% is due to random chance, and more likely that discrimination of promotions was occurring based on sex.

## Use the infer framework

### Randomized data under null model of independence

-   step through specifying the null model and then performing 1000 permutations to evaluate whether decision status differs between the "female" and "male" groups

-   specify() that the relationship of interest is decision vs. sex and a success in this context is promotion, set success to "promoted.

-   hypothesize() is used with null = "independence" for comparing difference of proportions, null  = "point", mu = VALUE for comparing difference of means.

```{r}
# Hypothesize independence (this test is used to compare difference of proportions)
decide_perm <- sex_disc |>
  specify(decision ~ sex, success = "promoted") |>  # syntax is y ~ x
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |> 
  calculate(stat = "diff in props", order = c("male", "female"))
decide_perm
```

## Calculate a p-value for the above hypothesis test for two-tail test

```{r}
p_value <- perm_1000_reps |>
  summarize(count = sum(abs(diff_orig) <= abs(diff_perm)), 
            proportion = mean(abs(diff_orig) <= abs(diff_perm)))
p_value
```

## If we wanted only a one-tail test, the resulting p-value is 1/2 the result above

```{r}
p_value <- perm_1000_reps |>
  summarize(count = sum(diff_orig <= diff_perm), 
            proportion = mean(diff_orig <= diff_perm))
p_value
```


## Conclusion based on p-value

This p-value suggests that such a difference from chance alone, assuming the null hypothesis was true, would be moderately rare: it would only happen about  44 in 1000 times. When results like these are inconsistent with Ho, we reject Ho in favor of Ha. Here, we concluded there was discrimination against female candidates. The 44-in-1000 chance is what we call a p-value, which is a probability quantifying the strength of the evidence against the null hypothesis, given the observed data.

## For future work

### The Chi Square Test

In the future, we will look at a different test, call the Chi Square Test, which also compares two groups proportions. If we look at the results of decision of promotions based on sex, the p-value is similar to our two-tailed simulation, at 0.05.


```{r}
chisq.test(sex_disc$decision, sex_disc$sex)
```


# Chapter 12 Confidence Intervals

## Confidence Intervals

Bootstrap percentile interval The main idea in the previous exercise was that the distance between the original sample p-hat.

And the resampled (or bootstrapped) p-hat values gives a measure for how far the original p-hat is from the true population proportion.

The same variability can be measured through a different mechanism. As before, if p-hat is sufficiently close to the true parameter, then the resampled (or bootstrapped) p-hat values will vary in such a way that they overlap with the true parameter.

### Create bootstrap distribution of proportion of females promoted

```{r}
# Compute p-hat* for each resampled proportion of females who were promoted
fprops <- sex_disc |>
  filter(sex == "female") |>
  # Specify vote as the response, where yes means success
  specify(response = decision, success = "promoted") |>
  # Generate 1000 reps of type bootstrap
  generate(reps = 1000, type = "bootstrap") |> 
  # Calculate the summary stat "prop"
  calculate(stat = "prop")
fprops
```

Find an interval of values that are plausible for the true parameter by calculating p-hatÂ±2SE

The lower bound of the confidence interval is p_hat minus twice the standard error of stat. Use sd() to calculate the standard error. The upper bound is p_hat plus twice the standard error of stat.

```{r}
# Manually calculate a 95% percentile interval
fprops |>
  summarize(
    lower = quantile(stat, p = 0.025),
    upper = quantile(stat, p = 0.975)
  )
```

```{r}
# Calculate the same interval, more conveniently
percentile_ci <- fprops |> 
  get_confidence_interval(level = 0.95)
percentile_ci
```

## Conclusion

We are 95% confident that the true proportion of women who are promoted is between 37.5% and 75.1%.

*Note - this is a wide range of values, so not very precise*
