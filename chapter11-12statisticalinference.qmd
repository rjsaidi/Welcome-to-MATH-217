---
title: "Chapter 11-12 - Statistical Inference"
author: "Rachel Saidi"
format: html
---

## Load library, set the working directory, and dataset

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(tidymodels)
setwd("C:/Users/rsaidi/Dropbox/Rachel/MontColl/Math217/Notes")
sex_disc <- read_csv("sex_discrimination.csv")
set.seed(9753)  # allows to replicate results each time document is rendered
```

```{r}
unique(sex_disc$decision)
```


## This vignette will help you understand HT using tidymodels

https://www.tidymodels.org/learn/statistics/infer/#:\~:text=hypothesize()%20allows%20you%20to,to%20form%20the%20null%20distribution.

## Two-Way table

```{r}
decision_table  <- table(sex_disc)
decision_table
```

## The kableExtra package allows for fancier tables

Use the kbl function to make a nice two-way table. row_spec() allows for adding attributes to the header columns in the table

```{r}
kbl(decision_table, caption = "Two-Way Table") |> 
  kable_styling() |> 
  row_spec(row = 0, color = "dodgerblue")

```

## This is the same table, but with proportions rounded to 3 decimals

```{r}
kbl(round(proportions(decision_table, "sex"),3)) |> # marginal proportions by sex for promotion decision rounded to 3 places
  kable_styling()|>
  row_spec(row = 0, color = "dodgerblue")
```

We can see that 10/24 (41.7%) women were not promoted and 3/24 (12.5%) men were not promoted. The question is, does this appear to be clear bias, or are the differences in proportions due to random chance?

## Random permutations

### One random permutation

Using the mutate() and sample() functions, the vector of promotion decision is mixed up, or permuted, such that whether someone is male or female can't possibly be causing any difference in proportions. However, due to inherent natural variability, there is also no expectation that the promotion decisions are exactly the same for any sample. We use the sample() function to create the shuffled promotion decisions, and then save that shuffled dataset into a new variable named decision_perm, using the mutate() function.

```{r}
perm1 <- sex_disc |>
  mutate(decision_perm = sample(decision)) |>
  group_by(sex) |>
  summarize(prop_promoted_perm = mean(decision_perm == "not promoted"),
            prop_promoted = mean(decision == "not promoted")) |>
  summarize(diff_perm = diff(prop_promoted_perm), 
            diff_orig = diff(prop_promoted))  # not promoted - promoted
perm1
```


```{r}
kbl(perm1)|>
  kable_styling() |>
  row_spec(row = 0, color = "dodgerblue")
```

We can see that the original (observed) difference in proportions was 41.7%-12.5%=29.2%

The permutated sample difference in proportions was different. Notice it will change each time you run this chunk due to random chance.

### Many random permutations

By repeating the permutation and difference calculations five times, the permuted differences are seen to be sometimes positive, sometimes negative, sometimes close to zero, sometimes far from zero. However, five times isn't quite enough to capture all of the variability in the null differences.

The rep_sample_n() function performs repeated sampling of a dataset, where the size of the sample is specified in the first argument and the number of repetitions is specified in the reps argument. We notice that the sample size is the same size as the sex discrimination dataset, since it equals the number of rows in the original dataset. We are also specifying that the sampling of the datset should be done without replacement (replace = FALSE), since we want each row to only be selected once. You can think of this as creating five copies of the original sex discrimination dataset.

```{r}
perm_reps <- sex_disc |>
  rep_sample_n(size = nrow(sex_disc), reps = 1000, replace = FALSE) |>
  mutate(decision_perm = sample(decision)) |>
  group_by(replicate, sex) |>
  summarize(prop_promoted_perm = mean(decision_perm == "not promoted"),
            prop_promoted = mean(decision == "not promoted")) |>
  summarize(diff_perm = diff(prop_promoted_perm), 
            diff_orig = diff(prop_promoted))  # not promoted - promoted
perm_reps
```

kbl(perm_reps) |>
  kable_styling() |>
    row_spec(row = 0, color = "dodgerblue")


*Notice the variation in the permutation differences.*

```{r}
ggplot(perm_reps, aes(diff_perm))+
  geom_histogram(bins = 25)
```

## Repeat the permutation 1000 times and plot all differences

```{r}
perm_1000_reps <- sex_disc |>
  rep_sample_n(size = nrow(sex_disc), reps = 1000, replace = FALSE) |>
  mutate(decision_perm = sample(decision)) |>
  group_by(replicate, sex) |>
  summarize(prop_promoted_perm = mean(decision_perm == "not promoted"),
            prop_promoted = mean(decision == "not promoted")) |>
  summarize(diff_perm = diff(prop_promoted_perm), 
            diff_orig = diff(prop_promoted))  # not promoted - promoted
```

## Create a dotplot of the replications of permutated differences

Show the cut-off for +/- the original difference in proportions of "promoted"

```{r}
plot_perm <- ggplot(perm_1000_reps, aes(diff_perm)) +
  geom_dotplot(binwidth = 0.01) +
  geom_vline(xintercept = c(-0.292, 0.292), color = "blue", linetype = "dotted", linewidth=1) +
  labs(title = "Dotplot of 1000 Replications of Permutations\n of Differences of Proportions")
plot_perm
```

Remember that each dot represents a different permutation of the differences in proportions of promotions between males and females.

Although the plot will appear differently each time due to randomization, notice that very few points are \> 0.292 or \< -0.292. Visually, this shows that it is less likely that the difference of 29.2% is due to random chance, and more likely that discrimination of promotions was occurring based on sex.

## Use the infer framework (tidymodels)

### Randomized data under null model of independence

-   step through specifying the null model and then performing 1000 permutations to evaluate whether decision status differs between the "female" and "male" groups

-   specify() that the relationship of interest is decision vs. sex and a success in this context is promotion, set success to "promoted.

-   hypothesize() is used with null = "independence" for comparing difference of proportions, null = "point", mu = VALUE for comparing difference of means.

```{r}
# Hypothesize independence (this test is used to compare difference of proportions)
decide_perm <- sex_disc |>
  specify(decision ~ sex, success = "not promoted") |>  # syntax is y ~ x
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |> 
  calculate(stat = "diff in props", order = c("female", "male"))
decide_perm
```

## Calculate a p-value for the above hypothesis test for two-tail test

I used the following to get more information about all of this code:

vignette("infer")

First set the observed difference in proportions value from the original data

```{r}
decide_obs <- sex_disc |>
  specify(decision ~ sex, success = "not promoted") |>  
  hypothesize(null = "independence") |>
  calculate(stat = "diff in props", order = c("female", "male")) 
decide_obs
```

## Now compute the p value using the function get_p_value

library(infer)

```{r}
#get_p_value
```


```{r}
pvalue <- get_p_value(decide_perm, decide_obs,  direction = "two-sided")
pvalue
```

## If we wanted only a one-tail test, the resulting p-value is 1/2 the result above

```{r}
.5*pvalue
```

```{r}
pvalue <- get_p_value(decide_perm, decide_obs,  direction = "greater")
pvalue
```


## Conclusion based on p-value

This p-value suggests that such a difference from chance alone, assuming the null hypothesis was true, would be moderately rare: it would only happen about 46 in 1000 times. When results like these are inconsistent with Ho, we reject Ho in favor of Ha. Here, we concluded there was discrimination against female candidates. The 46-in-1000 chance is what we call a p-value, which is a probability quantifying the strength of the evidence against the null hypothesis, given the observed data.

## Formulaic conclusion

1. p-value 0.046 is less than 0.05. 

2. Reject the null.  (if larger than 0.05, "Fail to reject the null.")

3. ** There is moderate evidence ** that there is discrimination in decisions to promote employees based on sex. 



## For future work

### The Chi Square Test

In the future, we will look at a different test, call the Chi Square Test, which also compares two groups proportions. If we look at the results of decision of promotions based on sex, the p-value is similar to our two-tailed simulation, at 0.05.

```{r}
chisq.test(sex_disc$decision, sex_disc$sex)
```

Please note that the chisq.test() gave a p-value 0.051, which is slightly larger than 0.05. What do we conclude???? 

Because 0.051 > 0.05, fail to reject the null. 

"There is no evidence that there is bias in decisions to promote based on sex."




## Using base R for prop.test()

You must use values from the original table cell counts 

```{r}
#prop.test(x, n, p = NULL, alternative = c("two.sided", "less", "greater"), conf.level = 0.95, correct = TRUE)
# for 2 groups, use c(x1,x2), c(n1,n2)
prop.test(c(10,3),c(24,24), alternative = "two.sided")
```



# Chapter 12 Confidence Intervals

## Confidence Intervals

Bootstrap percentile interval The main idea in the previous exercise was that the distance between the original sample p-hat.

And the resampled (or bootstrapped) p-hat values gives a measure for how far the original p-hat is from the true population proportion.

The same variability can be measured through a different mechanism. As before, if p-hat is sufficiently close to the true parameter, then the resampled (or bootstrapped) p-hat values will vary in such a way that they overlap with the true parameter.

### Create bootstrap distribution of single proportion of females not promoted

specify
generate
calculate

```{r}
# Compute p-hat* for each resampled proportion of females who were not promoted
fprops_boot <- sex_disc |>
  filter(sex == "female") |>
  # Specify vote as the response, where not promoted means success
  specify(response = decision, success = "not promoted") |>
  # Generate 1000 reps of type bootstrap
  generate(reps = 1000, type = "bootstrap") |> 
  # Calculate the summary stat "prop"
  calculate(stat = "prop")
fprops_boot
```

Find an interval of values that are plausible for the true parameter by calculating p-hatÂ±2SE

The lower bound of the confidence interval is p_hat minus twice the standard error of stat. Use sd() to calculate the standard error. The upper bound is p_hat plus twice the standard error of stat.

```{r}
fprops <- sex_disc |>
   filter(sex == "female") |>
  specify(response = decision, success = "not promoted") |>
  # Calculate the summary stat "prop"
  calculate(stat = "prop")
fprops  
```


```{r}
# Manually calculate a 95% percentile interval
fprops_boot |>
  summarize(
    lower = quantile(stat, p = 0.025),
    upper = quantile(stat, p = 0.975)
  )
```

```{r}
# Calculate the same interval, more conveniently
percentile_ci <- fprops_boot |> 
  get_confidence_interval(level = 0.90)
percentile_ci
```

## Conclusion

We are 95% confident that the true proportion of women who are not promoted is between 20.8% and 62.5%.

*Note - this is a wide range of values, so not very precise*


We are 90% confident that the true proportion of women who are not promoted is between 25% and 58.3%.
