---
title: "chapter 9 logistic regression"
author: Rachel Saidi
format: docx
---

## Load library tidyverse and resume dataset

![](Fall%202023/resume.JPG){width="198"}

Set your working directory to find the saved dataset and use "head" to view the variables in the dataset

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(openintro)
data(resume)
head(resume)
```

```{r}
unique(resume$received_callback)
```

## List all variables in the dataset

```{r}
names(resume)
```

## Visualizing the data

Let's start by looking at whether an applicant received a callback based on years experience.

## Ensure the response variable is binary

Check out **received_callback** to be sure that the values are zeros or ones. In our case, we are fine. If they are not, you can use the code below to change it.

data \<- data \|\> mutate(newvar = ifelse(oldvar == "yes", 1, 0))

Note that in the plot below, we use the **geom_jitter()** function to create the illusion of separation in our data. Because the y value is categorical, all of the points would either lie exactly on "dead" or "alive", making the individual points hard to see. To counteract this, geom_jitter() will move the points a small random amount up or down.

```{r}
ggplot(data = resume, aes(x = years_experience, y = received_callback)) + 
  geom_jitter(width = .05, height = 0.05, alpha = 0.3)
```

## Fit a logistic regression model

Start with the simple premise above, that years experience is associated with the likelihood of receiving a callback.

```{r}
plot_log <- ggplot(data = resume, aes(y = received_callback, x = years_experience)) + 
  geom_jitter(width = 0, height = 0.05, alpha = 0.5) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"))
plot_log
```

geom_smooth(method = "glm") creates a **sigmoid** curve, like a stretched-out S. This curve fits to the binary values (zeros and ones plotted). It shows the predicted probabilities of receiving a callback, based on years experience. You can see that even a candidate with 20 years of experience has about a 12.5% chance of receiving a callback.

## Odds scale

To combat the problem of the scale of the y variable, we can change the scale of the variable on the y-axis. Instead of thinking about the probability of receiving a callback, we can think about the odds. While these two concepts are often conflated, they are not the same. They are however, related by the simple formula below. The odds of a binary event are the ratio of how often it happens, to how often it doesn't happen.

odds(y-hat)= y-hat / (1−y-hat) = exp(β_0+β_1⋅x)

Thus, if the probability of receiving a callback is 10%, then the odds of receiving a callback are .1/(1-.1) = 1:9

Odds are commonly used to express uncertainty in a variety of contexts.

```{r}
# using base r logistic regression
#  model:  y ~ x
fit_log <- glm(data = resume, received_callback ~ years_experience, family = binomial())
summary(fit_log)
```

*equation log_odds(received_callback) = -2.7596+0.039(years_experience)*

*Note the AIC is 2714.2. We will try to improve that later (the smaller, the better).*

log_odds(callback_5years experience) = -2.7596 + 0.039(5)

```{r}
exp(-2.456)/(1+exp(-2.456))
  
```

# Use the **Tidymodels** package to create a regression model

## Create the training, testing, and validation data by splitting

Use initial_validation_split() function to create a random 3-way split the data into the

```{r}
# convert received_callback to categorical variable
resume$received_callback <- as.factor(resume$received_callback)
set.seed(123)

#initial_split creates a single binary split of the data into a training set and testing set (the default is 75% training 25% testing)
res_splits<- initial_validation_split(resume, strata = received_callback)

callback_train <- training(res_splits)
validation_set <- validation(res_splits)
callback_test  <- testing(res_splits)
```

## Calculate training set proportions by callbacks

```{r}
callback_train |>
  group_by(received_callback) |>
  tally() |>
  mutate(prop = n/sum(n))
```

We see that 7.9% of applicants in the training dataset received callbacks.

## Calculate training set proportions by callbacks

```{r}
validation_set |>
  group_by(received_callback) |>
  tally() |>
  mutate(prop = n/sum(n))
```

The proportion of callbacks in the validation set is 9.03%

## Calculate testing set proportions by callbacks

```{r}
callback_test |>
  group_by(received_callback) |>
  tally() |>
  mutate(prop = n/sum(n))
```

We see that in the testing set, 8.5% of applicants did receive callbacks.

For this case study, rather than using multiple iterations of resampling, let’s create a single resample called a validation set. In tidymodels, a validation set is treated as a single iteration of resampling. This will be a split from the 37,500 stays that were not used for testing, which we called hotel_other. This split creates two new datasets:

-   the set held out for the purpose of measuring performance, called the validation set

-   the remaining data used to fit the model, called the training set.

![](testing_training_validation.JPG){width="438"}

## Fitting Logistic Regression

You can fit any type of model (supported by tidymodels) using the following steps.

1.  Call the model function: here we called logistic_reg( ) as we want to fit a logistic regression model.

2.  Use set_engine( ) function to supply the family of the model. The “glm” argument as Logistic regression comes under the Generalized Linear Regression family.

3.  Use set_mode( ) function and supply the type of model you want to fit. Classify pos vs neg, so it is a classification.

4.  Next, you need to use the fit( ) function to fit the model and inside that in the form y\~x, you have to provide the formula notation and dataset (callback_train).

plus notation → diabetes \~ ind_variable 1 + ind_variable 2 + …….so on

```{r}
logit_fit <- 
  logistic_reg(mode = "classification") |>
  set_engine(engine = "glm") |> 
  set_mode("classification") |>
  fit(received_callback ~ years_experience, data = callback_train)
tidy(logit_fit)
```

## Odds Ratio

The interpretation of coefficients in the log-odds term does not make much sense if you need to report it in your article or publication. That is why the concept of odds ratio was introduced.

The ODDS is the ratio of the probability of an event occurring to the event not occurring. When we take a ratio of two such odds it called Odds Ratio.

you can get directly the odds ratios of the coefficient by supplying the exponentiate = True inside the tidy( ) function.

```{r}
tidy(logit_fit, exponentiate = TRUE)
```

## Model Prediction

Test Data Class Prediction

The very next step is to generate the test predictions that we could use for model evaluation. To generate the class prediction (pos/ neg) we can use the predict function and supply the trained model object, test dataset and the type which is here “class” as we want the class prediction, not probabilities.

# To get predicted values we use the function **predict**.

```{r}
pred_class <- predict(logit_fit,
                      new_data = callback_test,
                      type = "class")

pred_prob <- predict(logit_fit,
                      new_data = callback_test,
                      type = "prob")

joined_pred_results <- callback_test |>
  select(received_callback) |>
  bind_cols(pred_class, pred_prob)
```

## Model performance

**yardstick** is a package to get metrics on model performance. By default on classification problem yardstick::metrics returns accuracy and kappa.

https://yardstick.tidymodels.org/

```{r}
library(yardstick)
metrics(joined_pred_results, truth = received_callback, estimate = .pred_class)
```

According to Wikipedia, *accuracy* is how close a given set of measurements (observations or readings) are to their true value.

The accuracy of this model is 92.8%

Accuracy is calculated by:

(sensitivity+specificity)/(sensitivity+specificity+false_positive+false_negative)

## kappa

Kappa measures the model's reliability. It represents the extent to which the data collected in the study are correct representations of the variables measured.

Since kap = 0, the model does not appear to be a good representation.

## The null model

It will be useful to have our null model stored as a model object. We can create such an object using tidymodels by specifying a logistic regression model with **no explanatory variables**.

```{r}
null_mod <- 
  logistic_reg(mode = "classification") |>
  set_engine("glm") |>
  fit(received_callback ~ 1, data = callback_train)
tidy(null_mod)
```

## Compute the accuracy of the null model

```{r}
pred <- callback_train |>  # use the training dataset
  select(received_callback, years_experience) |> #select only these columns
  bind_cols(predict(null_mod, new_data = callback_train, type = "class"))|>
  rename(null_mod = .pred_class)

accuracy(pred, received_callback, null_mod)
```

The accuracy estimate of this null model is almost the same as the prior model: = 92.0%

## Create a confusion matrix

This is a two-way table that counts how often our model made the correct prediction. Note that there are two different types of mistakes that our model can make: predicting receiving a callback when a callback was not received (a Type I error), and predicting not receiving a callback when the callback was received (a Type II error).

```{r}
confusion_null <- pred |>
  conf_mat(truth = received_callback, estimate = null_mod)
confusion_null
```

Note that the null model predicts that *NONE* do not receive a callback (receives_callback = 0), so it makes many *Type II errors* (false negatives) and *NONE* receive a callback (receives_callback = 1) *Type I errors* (false positives).

## Build Logit Models and Predict

Start by building the logit model using training data

Beating the null model shouldn't be hard. Our first attempt will be to employ a simple logistic regression model. First, we'll fit the model using only one explanatory variable: years_experience. It stands to reason that more experienced people are more likely to get a job offer.

*This is the same as the first model we built in base R at the beginning.*

```{r}
mod_log_1 <- 
  logistic_reg(mode = "classification") |>
  set_engine("glm") |>
  fit(received_callback ~ years_experience, data = callback_train)
tidy(mod_log_1)
```

## Illustrate the predicted probability

Illustrate how the predicted probability of dying varies in our simple logistic regression model with respect to a person's age. Notice the sigmoid curve (in purple) - this is the shape of a logistic curve.

```{r}
train_plus <- callback_train |>
  mutate(called = as.integer(received_callback == 1))

ggplot(train_plus, aes(x = years_experience, y = called)) + 
  geom_count(
    position = position_jitter(width = 0, height = 0.05), 
    alpha = 0.5 ) + 
  geom_smooth(method = "glm", method.args = list(family = "binomial"), 
              color = "purple", lty = 2, se = FALSE) + 
  geom_hline(aes(yintercept = 0.5), linetype = 3) 
```

This still looks very similar to the original plot.

## ROC - Receiver Operating Characteristics

Receiver Operating Characteristics Curve traces the percentage of true positives accurately predicted by a given logit model as the prediction probability cutoff is lowered from 1 to 0. For a good model, as the cutoff is lowered, it should mark more of actual 1's as positives and lesser of actual 0's as 1's. So for a good model, the curve should rise steeply, indicating that the TPR (Y-Axis) increases faster than the FPR (X-Axis) as the cutoff score decreases.

## AUC - Area under the curve

The greater the area under the ROC curve, better the predictive ability of the model

```{r}
roc_auc(bind_cols(callback_test, pred_prob), truth = received_callback, .pred_1)

```

The AUC estimate is 43.3% - this is VERY low, meaning the model is not great for predicting probabilities.

```{r}
roc_data <- roc_curve(bind_cols(callback_test, pred_prob), truth = received_callback, .pred_1) 

roc_data |>  
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) + 
  coord_equal()+
  ggtitle("ROC Curve")+
  theme_bw()+
  geom_text(x=0.2, y=.8, label="AUC = 43%", color ="purple")
```

## The Full Regression Model

Computing the full logistic regression model will feel similar to the process for computing the full multiple linear regression model. We will use **GLM** to replace **LM** and we need to include **family = binomial()**

```{r}
log_mod_full <- glm(data=resume, received_callback ~ job_city + college_degree + years_experience + honors + military + has_email_address + race + gender + computer_skills + resume_quality, family = binomial())
summary(log_mod_full)
```

## Performance of Logistic Regression Model

To evaluate the performance of a logistic regression model, we must consider few metrics: AIC and p-values.

AIC (Akaike Information Criteria) -- The analogous metric of adjusted R² in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with **minimum** AIC value.

## Assessing our full model

1.  The AIC is 2678
2.  The largest p-values come from college_degree and resume_quality

## Remove college_degree and resume_quality and re-run the model

```{r}
log_mod2 <- glm(data=resume, received_callback ~ job_city + years_experience + honors + military + has_email_address + race + gender + computer_skills, family = binomial())
summary(log_mod2)
```

The AIC decreased to 2675 and all p-values are relatively low. This could possibly be our best model.

## Predicted  probability of receiving callbacks based on model predictors

The augment() function on the model object will return a data frame with—among other things—the fitted values, with .fitted. Be sure to include type.predict = "response"

```{r}
augment(log_mod2, type.predict = "response")
```


## VIF - Variance Inflation Factor

Like in case of linear regression, we should check for multicollinearity in the model.

One way to measure multicollinearity is the variance inflation factor (VIF), which assesses how much the variance of an estimated regression coefficient increases if your predictors are correlated.

A VIF between 5 and 10 indicates high correlation that may be problematic. And if the VIF goes above 10, you can assume that the regression coefficients are poorly estimated due to multicollinearity.

```{r message=FALSE}
#install.packages("car")  #car package will calculate the VIF
library(car)
vif(log_mod2)
```

All X variables in the model have VIF well below 4, so log_mod2 model does not appear to have multicollinearity.

## Example for predicting

Use this model(log_mod2) to estimate the probability of receiving a callback for: a job in Chicago where the candidate lists 14 years' experience, no honors, no military experience, includes an email address, has computer skills, and has a first name that implies they are a White male.

```{r}
# the log odds value of the above characteristics
-2.535-0.418+0.0196*14+0.269+0.437-0.239-0.242 

# calculate the probability using the above value
exp(-2.4536)/(1+exp(-2.4536))
```

The probability of receiving a callback for a candidate looking at a job in Chicago with 14 years' experience, no honors, no military experience, included an email address, has computer skills, and has a first name that implies they are White male is 7.92%

## Overall

Although this tutorial goes through many concepts that are involved with logistic regression, one of the take-aways is that there are many components involved in building the model and then testing the model.
