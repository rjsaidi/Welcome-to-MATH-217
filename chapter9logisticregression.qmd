---
title: "chapter 9 logistic regression"
author: Rachel Saidi
format: docx
toc: true
---

## Load library tidyverse and heart transplant dataset

[![https://www.mayoclinic.org/tests-procedures/heart-transplant/about/pac-20384750](heart%20transplant.JPG){width="483"}](https://www.mayoclinic.org/tests-procedures/heart-transplant/about/pac-20384750)

Load libraries and use "data" and "head" to view the variables in the dataset


```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(openintro)
data("heart_transplant")
head(heart_transplant)
heart <- heart_transplant #rename the dataset with a shorter name
```

## Ensure the response variable is binary

```{r}
unique(heart$survived)
```

## List all variables in the dataset

```{r}
names(heart) <- tolower(names(heart))
names(heart) <- gsub(" ", "", names(heart)) # removes spaces in spaces in variable names
```

## Visualizing the data

Let's start by looking at whether an applicant survived the transplant based on years.

Note that in the plot below, we use the **geom_jitter()** function to create the illusion of separation in our data. Because the y value is categorical, all of the points would either lie exactly on "dead" or "alive", making the individual points hard to see. To counteract this, geom_jitter() will move the points a small random amount up or down.

```{r}
ggplot(data = heart, aes(x = age, y = survived)) + 
  geom_jitter(width = .09, height = 0.09, alpha = 0.2)
```

## Make a binary variable

First, we have a technical problem, in that the levels of our response variable are labels, and you can’t build a regression model to a variable that consists of words! We can get around this by creating a new variable that is binary (either 0 or 1), based on whether the patient survived to the end of the study. We call this new variable is_alive.

```{r}
heart1 <- heart|>
  mutate(is_alive = ifelse(survived == "alive", 1, 0))
```

## Visualize a binary response

We can then visualize our data_space. The vertical axis can now be thought of as the probability of being alive at the end of the study, given one’s age at the beginning.

```{r}
ggplot(data = heart1, aes(x = age, y = is_alive)) + 
  geom_jitter(width = 0, height = 0.05, alpha = 0.5)
```

## Generalized linear models

Thankfully, a modeling framework exists that generalizes regression to include response variables that are non-normally distributed. This family is called generalized linear models or GLMs for short. One member of the family of GLMs is called logistic regression, and this is the one that models a binary response variable.

A full treatment of GLMs is beyond the scope of this tutorial, but the basic idea is that you apply a so-called link function to appropriately transform the scale of the response variable to match the output of a linear model. The link function used by logistic regression is the logit function. This constrains the fitted values of the model to always lie between 0 and 1, as a valid probability must.

In this lesson we will cover: - generalization of multiple regression - model non-normal responses - special case: logistic regression - models binary response - uses logit link function:

$logit(p) = log(p/(1-p)) = \beta_0 + \beta_1*x$

Fitting a GLM

********don't forget the "family = binomial**************

```{r}
fit1 <- glm(formula = is_alive ~ age, family = binomial(), data = heart1)
summary(fit1)
```

```{r}
plot_log <- ggplot(data = heart1, aes(y = is_alive, x = age)) + 
  geom_jitter(width = .05, height = 0.05, alpha = 0.2) +
  geom_smooth(method = "glm", color = "purple", method.args = list(family = "binomial"))
plot_log
```

geom_smooth(method = "glm") creates a **sigmoid** curve, like a stretched-out S. This curve fits to the binary values (zeros and ones plotted). It shows the predicted probabilities of being alive, based on age.

*As an example, a 60 year old patient has an approximately 12-13% probability of survival.*

The probability equation is:

$\hat{y} = \frac{exp(\hat{\beta_0}+\hat{\beta_1}*x)}{1+exp(\hat{\beta_0}+\hat{\beta_1}*x)}$

Here, we compute the fitted probabilities using the augment() function.

Set the type.predict argument to “response” to retrieve the **fitted values** on the familiar probability scale.

```{r}
heart_plus <- fit1 |>
  augment(type.predict = "response") |>
  mutate(y_hat = .fitted)
heart_plus
```

## Odds scale

To combat the problem of the scale of the y variable, we can change the scale of the variable on the y-axis. Instead of thinking about the probability of receiving a callback, we can think about the odds. While these two concepts are often conflated, they are not the same. They are however, related by the simple formula below. The odds of a binary event are the ratio of how often it happens, to how often it doesn't happen.

odds(y-hat)= y-hat / (1−y-hat) = exp(β_0+β_1⋅x)

$odds\hat{y} = \frac{hat{y}{1-hat{y}}$

Thus, if the probability of surviving is 70%, then the odds of surviving is .7/(1-.7) = .7/.3 = 7:3.

The probability scale is the easiest to understand, but it makes the logistic function difficult to interpret. Conversely the logistic function becomes a line on the log-odds scale. This makes the function easy to interpret, but the log of the odds is hard to grapple with. The odds scale lies somewhere in between.

## Odds Ratios

$OR = \frac{exp(\hat{\beta_0}+\hat{\beta_1}*(x+1))}{exp(\hat{\beta_0}+\hat{\beta_1}*x)} = exp\beta_1$

```{r}
exp(coef(fit1))
```

The equation of the model is:

log(odds)(is_alive) = 4.7797 + 0.9432(age).

## Example prediction

```{r}
age_70 <- data.frame(age = 70, transplant = "treatment")

augment(fit1, newdata = age_70, type.predict = "response")
```

*A 70 year old has a predicted probability of 7.39% survival five years after a heart transplant.*

## Confusion matrix

```{r}
fit_plus <- augment(fit1, type.predict = "response") |>
  mutate(alive_hat = round(.fitted))

fit_plus |>
  select(is_alive, alive_hat) |>
  table()
```

One common way of assessing performance of models for a categorical response is via a confusion matrix. This simply cross-tabulates the reality (is_alive) with what our model predicted (alive_hat). In this case, our model predicted that 96 patients would die, and only 7 would live. Of those 96, 71 actually did die, while of the 7, 3 actually lived. Thus, our overall accuracy was 74 out of 103, or about 72%.

# Use the **Tidymodels** package to create a regression model

## Create the training, testing, and validation data by splitting

Initial_split creates a single binary split of the data into a training set and testing set (the default is 75% training 25% testing)

```{r}
set.seed(123)

res_splits<- initial_split(heart1, strata = is_alive)

alive_train <- training(res_splits) #default 75% 
alive_test  <- testing(res_splits) # default 25%
```

## Calculate training set proportions by is_alive

```{r}
alive_train |>
  group_by(is_alive) |>
  tally() |>
  mutate(prop = n/sum(n))
```

*We can see that 72.8% are alive versus 27.2% are not alive 5 years after transplant surgery.*

*This is called "imbalanced", which can lead to biased results. A biased sample can often cause a great amount of disorder to the analysis.*

## Fitting Logistic Regression

You can fit any type of model (supported by tidymodels) using the following steps.

1.  Call the model function: here we called logistic_reg( ) as we want to fit a logistic regression model.

2.  Use set_engine( ) function to supply the family of the model. The “glm” argument as Logistic regression comes under the Generalized Linear Regression family.

3.  Next, you need to use the fit( ) function to fit the model and inside that in the form y\~x, you have to provide the formula notation and dataset (callback_train).

plus notation → diabetes \~ ind_variable 1 + ind_variable 2 + …….so on

```{r}
logit_fit_train <- 
  logistic_reg(mode = "classification") |>
  set_engine(engine = "glm") |> 
  fit(survived ~ age, data = alive_train)
logit_fit_train
```

*This model should be similar to the model we built using glm in the base R.*

## The AIC (Akaike information criterion): assesses the model in comparison to other models; the lower this value, the better the model.

*This first training model shows AIC: 81.76 which is better than the AIC of 117 from the full dataset.*

## Odds Ratio

The interpretation of coefficients in the log-odds term does not make much sense if you need to report it in your article or publication. That is why the concept of odds ratio was introduced.

The ODDS is the ratio of the probability of an event occurring to the event not occurring. When we take a ratio of two such odds it called Odds Ratio.

you can get directly the odds ratios of the coefficient by supplying the exponentiate = True inside the tidy( ) function.

```{r}
tidy(logit_fit_train, exponentiate = TRUE)
```

## Illustrate the predicted probability

Illustrate how the predicted probability of dying varies in our simple logistic regression model with respect to a person's age. Notice the sigmoid curve (in purple) - this is the shape of a logistic curve.

```{r}
ggplot(alive_train, aes(x = age, y = is_alive)) + 
  geom_count(
    position = position_jitter(width = 0, height = 0.05), 
    alpha = 0.5 ) + 
  geom_smooth(method = "glm", method.args = list(family = "binomial"), 
              color = "purple", lty = 2, se = FALSE) + 
  geom_hline(aes(yintercept = 0.5), linetype = 3) 
```

*This plot is very similar to the plot for the entire data.*

## Model Prediction

Test Data Class Prediction

The very next step is to generate the test predictions that we could use for model evaluation. To generate the class prediction (pos/ neg) we can use the predict function and supply the trained model object, test dataset and the type which is here “class” as we want the class prediction, not probabilities.

# To get predicted values we use the function **predict**.

```{r}
pred_class <- predict(logit_fit_train,
                      new_data = alive_test,
                      type = "class")

pred_prob <- predict(logit_fit_train,
                      new_data = alive_test,
                      type = "prob")

joined_pred_results <- alive_test |>
  select(survived, is_alive) |>
  bind_cols(pred_class, pred_prob)
```

## Model performance

**yardstick** is a package to get metrics on model performance. By default on classification problem yardstick::metrics returns accuracy and kappa.

https://yardstick.tidymodels.org/

```{r}
library(yardstick)
metrics(joined_pred_results, truth = survived, estimate = .pred_class)
```

*The accuracy is only 65.3% and the kappa is very low at basically zero. A kappa of 1 means perfect agreement with the two categorical classifications, and a kappa of 0 means no agreement (no better than random assignment).*

## The Confusion Matrix - Sensitivity and Specificity

One common way to evaluate the quality of a logistic regression model is to create a confusion matrix, which is a 2×2 table that shows the actual values from the model vs. the predicted values from the testing set.

```{r}
conf_mat(joined_pred_results, truth = survived, estimate = .pred_class)
```

```{r}
17/26
```


Find the sensitivity and specificity of the model.

```{r}
sens <- sens(joined_pred_results, truth = survived, estimate = .pred_class)
spec <- spec(joined_pred_results, truth = survived, estimate = .pred_class)
rbind(sens, spec)
```

The sensitivity is too low and the sum of sens and spec is less than 0.98. So, this model does NOT look good as inspected above.

## ROC Curve - Receiver Operator Characteristics

Now, we will use the ROC curve along with its corresponding AUC (area under the curve) for the testing set.

## ROC - Receiver Operating Characteristics

Receiver Operating Characteristics Curve traces the percentage of true positives accurately predicted by a given logit model as the prediction probability cutoff is lowered from 1 to 0. For a good model, as the cutoff is lowered, it should mark more of actual 1's as positives and lesser of actual 0's as 1's. So for a good model, the curve should rise steeply, indicating that the TPR (Y-Axis) increases faster than the FPR (X-Axis) as the cutoff score decreases.

## AUC - Area under the curve

The greater the area under the ROC curve, better the predictive ability of the model

```{r}
roc_auc(bind_cols(alive_test, pred_prob), survived, .pred_dead)

```

*The AUC estimate is 55% - meaning the model does a moderate job of predicting survival 5 years after a heart transplant, based on age.*

```{r}
roc_data <- roc_curve(bind_cols(alive_test, pred_prob), truth = survived, .pred_dead) 

roc_data |>  
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_smooth() +
  geom_abline(lty = 3) + 
  coord_equal()+
  ggtitle("ROC Curve")+
  theme_bw()+
  geom_text(x=0.2, y=.8, label="AUC = 53.7%", color ="purple")
```



## The Full Regression Model

Computing the full logistic regression model will feel similar to the process for computing the full multiple linear regression model. We will use **GLM** to replace **LM** and we need to include **family = binomial()**

```{r}
log_mod_full <- glm(data=alive_train, is_alive ~ acceptyear + age + survtime + prior + transplant, family = binomial())
summary(log_mod_full)
```


Ho: Survival is independent of the treatment or control
Ha: """"" is not independent """


```{r}
chisq.test(heart1$transplant, heart1$survived)
```





```{r}
log_mod_full <- glm(data=heart1, is_alive ~ acceptyear + age + survtime + prior + transplant, family = binomial())
summary(log_mod_full)
```



## Performance of Logistic Regression Model

To evaluate the performance of a logistic regression model, we must consider few metrics: AIC and p-values.

## Assessing our full model

1.  The AIC is 35.871 (recall it was 67.79 in the simple model with only age as a predictor)
2.  The largest p-values come from transplant and prior

## Remove transplant and prior and re-run the model

```{r}
log_mod2 <- glm(data=alive_train, is_alive ~ acceptyear + age + survtime, family = binomial()) 
summary(log_mod2)
```

*The AIC decreased to 36.278 and all p-values are relatively low.*

## VIF - Variance Inflation Factor

Like in case of linear regression, we should check for multicollinearity in the model.

One way to measure multicollinearity is the variance inflation factor (VIF), which assesses how much the variance of an estimated regression coefficient increases if your predictors are correlated.

A VIF between 5 and 10 indicates high correlation that may be problematic. And if the VIF goes above 10, you can assume that the regression coefficients are poorly estimated due to multicollinearity.

```{r message=FALSE}
#install.packages("car")  #car package will calculate the VIF
library(car)
vif(log_mod2)
```

Since acceptyear and survtime have VIF values \>5, remove one of the two.

```{r}
log_mod3 <- glm(data=alive_train, is_alive ~ acceptyear + age, family = binomial())
summary(log_mod3)
```

Although all variables have small p-values, the AIC went up.

*This tutorial does not come up with any definitive answer about a best model. Instead, it explores the many options and elements to include in a model and how to assess the model.*
